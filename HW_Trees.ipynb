{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Деревья решений решают проблемы\n",
    "__Суммарное количество баллов: 10__\n",
    "\n",
    "Вы уже знакомы с классификацией методом KNN. В этом задании предстоит реализовать другой метод классификации - дерево решений. \n",
    "\n",
    "Одной из его особенностей является возможность объяснить в человекочитаемой форме, почему мы отнесли объект к определенному классу. Эта особенность позволяет использовать деревья решений для создания систем, которые могут подсказывать специалистам, на что именно стоит обратить внимание при принятии решений."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from typing import Tuple"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Во время построения дерева решений нам потребуется определить, какой из предикатов лучше всего разбивает обучающую выборку. Есть два критерия, которые позволяют это сделать: критерий Джини и энтропийный критерий. Первый для подсчета информативности разбиения использует коэффициент Джини, второй - энтропию. Реализуйте подсчет этих коэффициентов, а так же подсчет информативности разбиения. \n",
    "\n",
    "#### Описание функций\n",
    "`gini(x)` считает коэффициент Джини для массива меток\n",
    "\n",
    "`entropy(x)` считает энтропию для массива меток\n",
    "\n",
    "`gain(left_y, right_y, criterion)` считает информативность разбиения массива меток на левую `left_y` и правую `right_y` части при помощи `criterion`, который задается функцией (не строкой)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from math import log\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "def get_p(x):\n",
    "    def tmp(clazz):\n",
    "        return len([ t for t in x if t == clazz]) / len(x)\n",
    "    \n",
    "    return tmp\n",
    "\n",
    "def gini(x):\n",
    "    classes = np.unique(x)\n",
    "\n",
    "    p = get_p(x)\n",
    "\n",
    "    return reduce(\n",
    "        lambda acc, clazz: acc + p(clazz) * (1 - p(clazz)),\n",
    "        classes,\n",
    "        0\n",
    "    )\n",
    "    \n",
    "def entropy(x):\n",
    "    classes = np.unique(x)\n",
    "\n",
    "    p = get_p(x)\n",
    "\n",
    "    return -reduce(\n",
    "        lambda acc, clazz: acc + p(clazz) * log(p(clazz)),\n",
    "        classes,\n",
    "        0\n",
    "    )\n",
    "\n",
    "def gain(left_y, right_y, criterion):\n",
    "    return (len(left_y) * criterion(left_y) + len(right_y) * criterion(right_y)) / (len(left_y) + len(right_y))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Деревья решений имеют хорошую интерпретируемость, т.к. позволяют не только предсказать класс, но и объяснить, почему мы предсказали именно его. Например, мы можем его нарисовать. Чтобы сделать это, нам необходимо знать, как оно устроено внутри. Реализуйте классы, которые будут задавать структуру дерева. \n",
    "\n",
    "#### DecisionTreeLeaf\n",
    "Поля:\n",
    "1. `y` должно содержать класс, который встречается чаще всего среди элементов листа дерева\n",
    "\n",
    "#### DecisionTreeNode\n",
    "В данной домашней работе мы ограничемся порядковыми и количественными признаками, поэтому достаточно хранить измерение и значение признака, по которому разбиваем обучающую выборку.\n",
    "\n",
    "Поля:\n",
    "1. `split_dim` измерение, по которому разбиваем выборку\n",
    "2. `split_value` значение, по которому разбираем выборку\n",
    "3. `left` поддерево, отвечающее за случай `x[split_dim] < split_value`. Может быть `DecisionTreeNode` или `DecisionTreeLeaf`\n",
    "4. `right` поддерево, отвечающее за случай `x[split_dim] >= split_value`. Может быть `DecisionTreeNode` или `DecisionTreeLeaf`\n",
    "\n",
    "__Интерфейс классов можно и нужно менять при необходимости__ (например, для вычисления вероятности в следующем задании)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DecisionTreeLeaf:\n",
    "    def __init__(self, labels):\n",
    "        self.probabilities = {}\n",
    "        unique_labels = np.unique(labels)\n",
    "        \n",
    "        for label in unique_labels:\n",
    "            self.probabilities[label] = len([ y for y in labels if y == label ]) / len(labels)\n",
    "        \n",
    "        self.y = reduce(\n",
    "            lambda max_label, current_label: current_label if self.probabilities[max_label] < self.probabilities[current_label] else max_label,\n",
    "            self.probabilities\n",
    "        )\n",
    "\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self, split_dim, split_value, left, right):\n",
    "        self.split_dim = split_dim\n",
    "        self.split_value = split_value\n",
    "        self.left = left\n",
    "        self.right = right"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь перейдем к самому дереву решений. Реализуйте класс `DecisionTreeClassifier`.\n",
    "\n",
    "#### Описание методов\n",
    "`fit(X, y)` строит дерево решений по обучающей выборке.\n",
    "\n",
    "`predict_proba(X)` для каждого элемента из `X` возвращает словарь `dict`, состоящий из пар `(класс, вероятность)`. Вероятности классов в листе можно определить через количество объектов соответствующего класса в листе. \n",
    "\n",
    "#### Описание параметров конструктора\n",
    "`criterion=\"gini\"` - задает критерий, который будет использоваться при построении дерева. Возможные значения: `\"gini\"`, `\"entropy\"`.\n",
    "\n",
    "`max_depth=None` - ограничение глубины дерева. Если `None` - глубина не ограничена\n",
    "\n",
    "`min_samples_leaf=1` - минимальное количество элементов в каждом листе дерева.\n",
    "\n",
    "#### Описание полей\n",
    "`root` - корень дерева. Может быть `DecisionTreeNode` или `DecisionTreeLeaf`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, criterion=\"gini\", max_depth=None, min_samples_leaf=1):\n",
    "        self.root = None\n",
    "        self.__gain = lambda left, right: gain(left, right, gini if criterion == 'gini' else entropy)\n",
    "        self.__max_depth = max_depth\n",
    "        self.__min_samples_leaf = min_samples_leaf\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.__data = {} # Словарь: Ключ -- id, значение -- пара (пример, метка)\n",
    "        \n",
    "        for x in X:\n",
    "            label = -1\n",
    "            \n",
    "            for i in y:\n",
    "                if i[0] == x[0]:\n",
    "                    label = i[1]\n",
    "                    break\n",
    "            \n",
    "            if label != -1:\n",
    "                self.__data[x[0]] = (x[1:], label)\n",
    "        \n",
    "        self.root = self.__get_node_or_leaf(self.__data, 1)\n",
    "                    \n",
    "    def __get_node_or_leaf(self, data_subset, depth):\n",
    "        labels = [ data_subset[id][1] for id in data_subset ]\n",
    "        \n",
    "        if len(data_subset.items()) <= self.__min_samples_leaf:\n",
    "            return DecisionTreeLeaf(labels)\n",
    "        \n",
    "        if self.__max_depth is not None and self.__max_depth <= depth: # Дререво достигло максимальной глубины\n",
    "            return DecisionTreeLeaf(labels)\n",
    "        \n",
    "        if len(np.unique(labels)) == 1: # Все объекты принадлежат к одному классу\n",
    "            return DecisionTreeLeaf(labels)\n",
    "        \n",
    "\n",
    "        gain = None # Вместо +inf\n",
    "        split_feature_idx = 0\n",
    "        split_treshold = 0\n",
    "        left_data_subset, right_data_subset = {}, {}\n",
    "\n",
    "        a = np.empty((len(data_subset), len(next(iter(data_subset.values()))[0])))\n",
    "        print('shape', a.shape)\n",
    "        counter = 0\n",
    "        for id in data_subset:\n",
    "            a[counter] = data_subset[id][0]\n",
    "            counter+=1\n",
    "        print(a.reshape((-1, len(data_subset))))\n",
    "        for feature_idx, feature in enumerate(next(iter(data_subset.values()))[0]): # Достаём первый элемент из словаря\n",
    "            pass\n",
    "        \n",
    "        for sample_id in data_subset:\n",
    "            sample_with_label = data_subset[sample_id]\n",
    "            for feature_id, feature_value in enumerate(sample_with_label[0]):                \n",
    "\n",
    "                samples_with_labels_left, samples_with_labels_right = {}, {}\n",
    "\n",
    "                for id in data_subset:\n",
    "                    if data_subset[id][0][feature_id] < feature_value:\n",
    "                        samples_with_labels_left[id] = data_subset[id]\n",
    "                    else:\n",
    "                        samples_with_labels_right[id] = data_subset[id]\n",
    "\n",
    "                local_gain = self.__gain(\n",
    "                    [ data_subset[id][1] for id in samples_with_labels_left ],\n",
    "                    [ data_subset[id][1] for id in samples_with_labels_right ]\n",
    "                )\n",
    "                \n",
    "                if gain is None or local_gain < gain:\n",
    "                    gain = local_gain\n",
    "                    split_feature_idx = feature_id\n",
    "                    split_treshold = feature_value\n",
    "                    left_data_subset = samples_with_labels_left.copy()\n",
    "                    right_data_subset = samples_with_labels_right.copy()\n",
    "        \n",
    "        return DecisionTreeNode(\n",
    "            split_feature_idx,\n",
    "            split_treshold,\n",
    "            self.__get_node_or_leaf(left_data_subset, depth + 1),\n",
    "            self.__get_node_or_leaf(right_data_subset, depth + 1)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        result = np.empty(len(X), dtype=object)\n",
    "\n",
    "        for sample_idx, sample in enumerate(X):\n",
    "            current_node = self.root\n",
    "\n",
    "            while type(current_node) != type(DecisionTreeLeaf([0])): # Аргумент, передоваемый в Leaf -- костыль\n",
    "                split_feature_idx = current_node.split_dim + 1\n",
    "\n",
    "                if sample[split_feature_idx] < current_node.split_value:\n",
    "                    current_node = current_node.left\n",
    "                else:\n",
    "                    current_node = current_node.right\n",
    "            \n",
    "            result[sample_idx] = current_node.probabilities\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return [max(p.keys(), key=lambda k: p[k]) for p in proba]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Построенное дерево можно нарисовать. Метод `draw_tree` рисует дерево и сохраняет его в указанный файл."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def tree_depth(tree_root):\n",
    "    if isinstance(tree_root, DecisionTreeNode):\n",
    "        return max(tree_depth(tree_root.left), tree_depth(tree_root.right)) + 1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def draw_tree_rec(tree_root, x_left, x_right, y):\n",
    "    x_center = (x_right - x_left) / 2 + x_left\n",
    "    if isinstance(tree_root, DecisionTreeNode):\n",
    "        x_center = (x_right - x_left) / 2 + x_left\n",
    "        x = draw_tree_rec(tree_root.left, x_left, x_center, y - 1)\n",
    "        plt.plot((x_center, x), (y - 0.1, y - 0.9), c=(0, 0, 0))\n",
    "        x = draw_tree_rec(tree_root.right, x_center, x_right, y - 1)\n",
    "        plt.plot((x_center, x), (y - 0.1, y - 0.9), c=(0, 0, 0))\n",
    "        plt.text(x_center, y, \"x[%i] < %f\" % (tree_root.split_dim, tree_root.split_value),\n",
    "                horizontalalignment='center')\n",
    "    else:\n",
    "        plt.text(x_center, y, str(tree_root.y),\n",
    "                horizontalalignment='center')\n",
    "    return x_center\n",
    "\n",
    "def draw_tree(tree, save_path=None):\n",
    "    td = tree_depth(tree.root)\n",
    "    plt.figure(figsize=(0.33 * 2 ** td, 2 * td))\n",
    "    plt.xlim(-1, 1)\n",
    "    plt.ylim(0.95, td + 0.05)\n",
    "    plt.axis('off')\n",
    "    draw_tree_rec(tree.root, -1, 1, td)\n",
    "    plt.tight_layout()\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для двумерного набора данных дерево можно отобразить на плоскости с данными. Кроме того, как и для любого классификатора, для него можно построить roc-кривую"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_roc_curve(y_test, p_pred):\n",
    "    positive_samples = sum(1 for y in y_test if y == 0)\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    for w in np.arange(-0.01, 1.02, 0.01):\n",
    "        y_pred = [(0 if p.get(0, 0) > w else 1) for p in p_pred]\n",
    "        tpr.append(sum(1 for yp, yt in zip(y_pred, y_test) if yp == 0 and yt == 0) / positive_samples)\n",
    "        fpr.append(sum(1 for yp, yt in zip(y_pred, y_test) if yp == 0 and yt != 0) / (len(y_test) - positive_samples))\n",
    "    plt.figure(figsize = (7, 7))\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False positive rate\")\n",
    "    plt.ylabel(\"True positive rate\")\n",
    "    plt.xlim(-0.01, 1.01)\n",
    "    plt.ylim(-0.01, 1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def rectangle_bounds(bounds):\n",
    "    return ((bounds[0][0], bounds[0][0], bounds[0][1], bounds[0][1]), \n",
    "            (bounds[1][0], bounds[1][1], bounds[1][1], bounds[1][0]))\n",
    "\n",
    "def plot_2d_tree(tree_root, bounds, colors):\n",
    "    if isinstance(tree_root, DecisionTreeNode):\n",
    "        if tree_root.split_dim:\n",
    "            plot_2d_tree(tree_root.left, [bounds[0], [bounds[1][0], tree_root.split_value]], colors)\n",
    "            plot_2d_tree(tree_root.right, [bounds[0], [tree_root.split_value, bounds[1][1]]], colors)\n",
    "            plt.plot(bounds[0], (tree_root.split_value, tree_root.split_value), c=(0, 0, 0))\n",
    "        else:\n",
    "            plot_2d_tree(tree_root.left, [[bounds[0][0], tree_root.split_value], bounds[1]], colors)\n",
    "            plot_2d_tree(tree_root.right, [[tree_root.split_value, bounds[0][1]], bounds[1]], colors)\n",
    "            plt.plot((tree_root.split_value, tree_root.split_value), bounds[1], c=(0, 0, 0))\n",
    "    else:\n",
    "        x, y = rectangle_bounds(bounds)\n",
    "        plt.fill(x, y, c=colors[tree_root.y] + [0.2])\n",
    "\n",
    "def plot_2d(tree, X, y):\n",
    "    plt.figure(figsize=(9, 9))\n",
    "    colors = dict((c, list(np.random.random(3))) for c in np.unique(y))\n",
    "    bounds = list(zip(np.min(X, axis=0), np.max(X, axis=0)))\n",
    "    plt.xlim(*bounds[0])\n",
    "    plt.ylim(*bounds[1])\n",
    "    plot_2d_tree(tree.root, list(zip(np.min(X, axis=0), np.max(X, axis=0))), colors)\n",
    "    for c in np.unique(y):\n",
    "        plt.scatter(X[y==c, 0], X[y==c, 1], c=[colors[c]], label=c)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Протестируйте решение на датасете spam.\n",
    "Для этой задачи используйте данные x_spam_train и y_spam_train:\n",
    "1. Выполните загрузку и предобработку файлов x_spam_train и y_spam_train.\n",
    "2. Разбейте x_spam_train и y_spam_train на x_train, y_train, x_test и y_test для оценки точности работы алгоритма.\n",
    "3. Посчитайте метрики `precision`, `recall`, `accuracy` для модели Decision Tree. Если необходимо, попробуйте разные наборы параметров для получения лучшего результата.\n",
    "4. Сравните значения метрик с результатами модели kNN из предыдущего задания (можно использовать реализацию из библиотеки `sklearn`).\n",
    "5. Ответьте на следующие вопросы:\n",
    "    - Какой нужен препроцессинг данных для моделей?\n",
    "    - Какая модель делает предсказания лучше?  Предположите, почему."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Ваш ответ_"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_test_split(X: pd.DataFrame, y: np.array, ratio: float = 0.7) -> Tuple[np.array, np.array, np.array, np.array]:\n",
    "    # Возвращает X_train, y_train, X_test, y_test\n",
    "    # X_train и X_test - массив векторов - две части массива X, разделенного в состветсви с коэффициентом ratio\n",
    "    # y_train и y_test - соответствующие X_train и X_test метки классов\n",
    "    treshold: int = int(X.shape[0] * ratio)\n",
    "    return X[:treshold].to_numpy(), y[:treshold], X[treshold:].to_numpy(), y[treshold:]\n",
    "\n",
    "X, y = pd.read_csv('hw_trees_data/x_spam_train.csv'), pd.read_csv('hw_trees_data/y_spam_train.csv').to_numpy()\n",
    "X_train, y_train, X_test, y_test = train_test_split(X, y, ratio=0.3)\n",
    "tree = DecisionTreeClassifier(max_depth=5, min_samples_leaf=30)\n",
    "tree.fit(X_train, y_train)\n",
    "plot_roc_curve([ y[1] for y in y_test ], tree.predict_proba(X_test))\n",
    "draw_tree(tree)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Обучите модель на всех данных из x_spam_train и y_spam_train.\n",
    "2. Сделайте submit своего решения и получите значение f1_score не менее 0.6"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "submission = pd.DataFrame(columns = [\"Id\", \"Expected\"])\n",
    "submission[\"Id\"] = test[\"Id\"]\n",
    "submission[\"Expected\"] = #YOUR CODE\n",
    "submission.to_csv('submission.csv', index=False)\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('env': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "8d22865cb200c2dfdd3f5854500180a39270a2dc4977b5698ff256ed7c9469f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
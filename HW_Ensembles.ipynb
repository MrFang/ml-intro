{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Случайные леса\n",
    "__Суммарное количество баллов: 10__\n",
    "\n",
    "В этом задании вам предстоит реализовать ансамбль деревьев решений, известный как случайный лес, применить его к публичным данным и сравнить его эффективность с ансамблями из самых популярных библиотек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем сам Random Forest. Идея очень простая: строим `n` деревьев, а затем берем модальное предсказание. Используйте реализацию дерева из HW3.\n",
    "\n",
    "#### Параметры конструктора\n",
    "`n_estimators` - количество используемых для предсказания деревьев.\n",
    "\n",
    "Остальное - параметры деревьев.\n",
    "\n",
    "#### Методы\n",
    "`fit(X, y)` - строит `n_estimators` деревьев по выборке `X`.\n",
    "\n",
    "`predict(X)` - для каждого элемента выборки `X` возвращает самый частый класс, который предсказывают для него деревья."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from math import log\n",
    "\n",
    "def get_p(x):\n",
    "    def tmp(clazz):\n",
    "        return len([ t for t in x if t == clazz]) / len(x)\n",
    "    \n",
    "    return tmp\n",
    "\n",
    "def gini(x):\n",
    "    classes = np.unique(x)\n",
    "\n",
    "    p = get_p(x)\n",
    "\n",
    "    return reduce(\n",
    "        lambda acc, clazz: acc + p(clazz) * (1 - p(clazz)),\n",
    "        classes,\n",
    "        0\n",
    "    )\n",
    "    \n",
    "def entropy(x):\n",
    "    classes = np.unique(x)\n",
    "\n",
    "    p = get_p(x)\n",
    "\n",
    "    return -reduce(\n",
    "        lambda acc, clazz: acc + p(clazz) * log(p(clazz)),\n",
    "        classes,\n",
    "        0\n",
    "    )\n",
    "\n",
    "def gain(left_y, right_y, criterion):\n",
    "    return (len(left_y) * criterion(left_y) + len(right_y) * criterion(right_y)) / (len(left_y) + len(right_y))\n",
    "\n",
    "class DecisionTreeLeaf:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        labels = [ data[id][1] for id in data ]\n",
    "        self.probabilities = {}\n",
    "        unique_labels = np.unique(labels)\n",
    "        \n",
    "        for label in unique_labels:\n",
    "            self.probabilities[label] = len([ y for y in labels if y == label ]) / len(labels)\n",
    "        \n",
    "        self.y = reduce(\n",
    "            lambda max_label, current_label: current_label if self.probabilities[max_label] < self.probabilities[current_label] else max_label,\n",
    "            self.probabilities,\n",
    "        )\n",
    "\n",
    "\n",
    "class DecisionTreeNode(DecisionTreeLeaf):\n",
    "    # Дефолтные значения аргументов, чтобы можно было вызвать пустой конструктор для проверки типов\n",
    "    def __init__(self, split_dim = None, split_value = None, left = None, right = None):\n",
    "        if split_dim is None or split_value is None or left is None or right is None:\n",
    "            return\n",
    "        \n",
    "        super().__init__({**left.data, **right.data})\n",
    "        self.split_dim = split_dim\n",
    "        self.split_value = split_value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, criterion=\"gini\", max_depth=None, min_samples_leaf=1):\n",
    "        self.root = None\n",
    "        self.__gain = lambda left, right: gain(left, right, gini if criterion == 'gini' else entropy)\n",
    "        self.__max_depth = max_depth\n",
    "        self.__min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.__data = {} # Словарь: Ключ -- id, значение -- пара (пример, метка)\n",
    "        \n",
    "        for x in X: # Заполняем self.__data\n",
    "            label = -1\n",
    "            \n",
    "            for i in y:\n",
    "                if i[0] == x[0]:\n",
    "                    label = i[1]\n",
    "                    break\n",
    "            \n",
    "            if label != -1:\n",
    "                self.__data[x[0]] = (x[1:], label)\n",
    "        \n",
    "        # Для каждой фичи определяем \"границы\" минимум и максимум, которого она достигает на датасете\n",
    "        tmp = np.empty((len(self.__data), len(next(iter(self.__data.values()))[0]))) # Размеры tmp -- (<размер датасета>, <количество фич первого элемента словаря>)\n",
    "\n",
    "        counter = 0;\n",
    "        \n",
    "        for id in self.__data:\n",
    "            tmp[counter] = self.__data[id][0]\n",
    "            counter += 1\n",
    "\n",
    "        self.__borders = np.transpose(np.array([np.amin(tmp, axis=0), np.amax(tmp, axis = 0)]))\n",
    "\n",
    "        self.root = self.__get_node_or_leaf(self.__data, 1)\n",
    "                    \n",
    "    def __get_node_or_leaf(self, data_subset, depth):\n",
    "        labels = [ data_subset[id][1] for id in data_subset ]\n",
    "        \n",
    "        if len(data_subset.items()) == self.__min_samples_leaf:\n",
    "            return DecisionTreeLeaf(data_subset)\n",
    "        \n",
    "        if self.__max_depth is not None and self.__max_depth <= depth: # Дререво достигло максимальной глубины\n",
    "            return DecisionTreeLeaf(data_subset)\n",
    "        \n",
    "        if len(np.unique(labels)) == 1: # Все объекты принадлежат к одному классу\n",
    "            return DecisionTreeLeaf(data_subset)\n",
    "        \n",
    "\n",
    "        gain = None # Вместо +inf\n",
    "        split_feature_idx = 0\n",
    "        split_treshold = 0\n",
    "        left_data_subset, right_data_subset = {}, {}\n",
    "        n_features = len( next( iter( data_subset.values() ) )[0] )\n",
    "        \n",
    "        for feature_id in np.random.choice(\n",
    "            np.arange(n_features),\n",
    "            n_features if self.max_features is None else self.max_features,\n",
    "            replace=False\n",
    "        ): # Выбираем __max_features по которым считаем gain\n",
    "\n",
    "            feature_min, feature_range = \\\n",
    "                self.__borders[feature_id][0], \\\n",
    "                self.__borders[feature_id][1] - self.__borders[feature_id][0]\n",
    "\n",
    "            for i in range(0, 101): # Делим облазть значений каждой фичи на 100 кусков и выполняем разбиения по границам этих кусков\n",
    "                feature_value = feature_min + (feature_range / 100 * i)\n",
    "                samples_with_labels_left, samples_with_labels_right = {}, {}\n",
    "\n",
    "                for id in data_subset:\n",
    "                    if data_subset[id][0][feature_id] < feature_value:\n",
    "                        samples_with_labels_left[id] = data_subset[id]\n",
    "                    else:\n",
    "                        samples_with_labels_right[id] = data_subset[id]\n",
    "                \n",
    "                if len(samples_with_labels_left.items()) < self.__min_samples_leaf or len(samples_with_labels_right.items()) < self.__min_samples_leaf:\n",
    "                    continue\n",
    "\n",
    "                local_gain = self.__gain(\n",
    "                    [ data_subset[id][1] for id in samples_with_labels_left ],\n",
    "                    [ data_subset[id][1] for id in samples_with_labels_right ]\n",
    "                )\n",
    "                \n",
    "                if gain is None or local_gain < gain:\n",
    "                    gain = local_gain\n",
    "                    split_feature_idx = feature_id\n",
    "                    split_treshold = feature_value\n",
    "                    left_data_subset = samples_with_labels_left.copy()\n",
    "                    right_data_subset = samples_with_labels_right.copy()\n",
    "        \n",
    "        return DecisionTreeNode(\n",
    "            split_feature_idx,\n",
    "            split_treshold,\n",
    "            self.__get_node_or_leaf(left_data_subset, depth + 1),\n",
    "            self.__get_node_or_leaf(right_data_subset, depth + 1)\n",
    "        ) if gain is not None else DecisionTreeLeaf(data_subset)\n",
    "\n",
    "    \n",
    "    def predict_proba(self, X, max_depth=None):\n",
    "        result = np.empty(len(X), dtype=object)\n",
    "\n",
    "        for sample_idx, sample in enumerate(X):\n",
    "            current_node = self.root\n",
    "\n",
    "            depth = 1\n",
    "            while type(current_node) == type(DecisionTreeNode()) and (max_depth is not None and depth < max_depth):\n",
    "                split_feature_idx = current_node.split_dim + 1 # +1 потому что при обучении мы проигнорировали колонку ID, поэтому все индексы в узлах на единицу меньше\n",
    "\n",
    "                if sample[split_feature_idx] < current_node.split_value:\n",
    "                    current_node = current_node.left\n",
    "                else:\n",
    "                    current_node = current_node.right\n",
    "                \n",
    "                depth += 1\n",
    "            \n",
    "            result[sample_idx] = current_node.probabilities\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def predict(self, X, max_depth=None):\n",
    "        proba = self.predict_proba(X, max_depth)\n",
    "        return [ max(p.keys(), key=lambda k: p[k]) for p in proba ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "class RandomForestClassifier:\n",
    "    def __init__(self, criterion=\"gini\", max_depth=None, min_samples_leaf=1, max_features=\"auto\", n_estimators=10):\n",
    "        self.__max_features = max_features\n",
    "        self.__n_estimators = n_estimators\n",
    "        self.__criterion = criterion\n",
    "        self.__max_depth = max_depth\n",
    "        self.__min_samples_leaf = min_samples_leaf\n",
    "        self.__trees = np.array([ DecisionTreeClassifier(\n",
    "            criterion=criterion,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_leaf=min_samples_leaf\n",
    "        ) for _ in range(n_estimators) ])     \n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.__classes = np.unique([ i[1] for i in y ])\n",
    "        self.__oob_data = []\n",
    "        bagged_data = self.__get_bagged_datasets(X, y, self.__n_estimators, len(X))\n",
    "\n",
    "        for tree_idx in range(self.__n_estimators):\n",
    "            self.__trees[tree_idx].max_features = int(sqrt(X.shape[1])) if self.__max_features == 'auto' else self.__max_features\n",
    "            self.__oob_data.append((bagged_data[tree_idx][2], bagged_data[tree_idx][3]))\n",
    "\n",
    "            self.__trees[tree_idx].fit(bagged_data[tree_idx][0], bagged_data[tree_idx][1])\n",
    "    \n",
    "    \n",
    "    def predict_proba(self, X, n_trees=None, max_depth=None):\n",
    "        n_estimators = self.__n_estimators if n_trees is None else n_trees\n",
    "        predictions = np.empty((n_estimators, len(X)), dtype=int)\n",
    "\n",
    "        probabilities = np.empty(len(X), dtype=object)\n",
    "        \n",
    "        for tree_idx in range(n_estimators):\n",
    "            predictions[tree_idx] = self.__trees[tree_idx].predict(X, max_depth)\n",
    "        \n",
    "        predictions = np.transpose(predictions)\n",
    "\n",
    "        for prediction_idx, prediction in enumerate(predictions):\n",
    "            prob = {}\n",
    "\n",
    "            for clazz in self.__classes:\n",
    "                prob[clazz] = len([ 1 for p in prediction if p == clazz ]) / n_estimators\n",
    "            \n",
    "            probabilities[prediction_idx] = prob;\n",
    "\n",
    "        return probabilities\n",
    "\n",
    "    \n",
    "    def predict(self, X, n_trees=None, max_depth=None):\n",
    "        proba = self.predict_proba(X, n_trees, max_depth)\n",
    "        return [ max(p.keys(), key=lambda k: p[k]) for p in proba ]\n",
    "\n",
    "    \n",
    "    def compute_oob_error(self, n_trees=None, max_depth=None):\n",
    "        n_estimators = self.__n_estimators if n_trees is None else n_trees\n",
    "        oob_error = np.empty((n_estimators), dtype=int)\n",
    "        \n",
    "        for tree_idx in range(n_estimators):\n",
    "            prediction = self.__trees[tree_idx].predict(self.__oob_data[tree_idx][0], max_depth)\n",
    "            oob_error[tree_idx] = sum([ 1 for idx in range(len(prediction)) if prediction[idx] != self.__oob_data[tree_idx][1][idx][1]])\n",
    "        \n",
    "        return np.mean(oob_error)\n",
    "\n",
    "    \n",
    "    def clone(self):\n",
    "        return RandomForestClassifier(\n",
    "            self.__criterion,\n",
    "            self.__max_depth,\n",
    "            self.__min_samples_leaf,\n",
    "            self.__max_features,\n",
    "            self.__n_estimators\n",
    "        )\n",
    "    \n",
    "    def __get_bagged_datasets(self, X, y, number_of_sets, number_of_samples):\n",
    "        X_sets = np.empty((number_of_sets, number_of_samples, X.shape[1]))\n",
    "        y_sets = np.empty((number_of_sets, number_of_samples, y.shape[1]))\n",
    "        oob_X_sets = []\n",
    "        oob_y_sets = []\n",
    "        \n",
    "        for set_number in range(number_of_sets):\n",
    "            indexes = np.arange(X.shape[0])\n",
    "            chosen_indexes = np.random.choice(indexes, number_of_samples, replace=True)\n",
    "            X_sets[set_number] = X[chosen_indexes]\n",
    "            y_sets[set_number] = y[chosen_indexes]\n",
    "            oob_X_sets.append(X[np.delete(indexes, chosen_indexes)])\n",
    "            oob_y_sets.append(y[np.delete(indexes, chosen_indexes)])\n",
    "        \n",
    "        return [ (X_sets[idx], y_sets[idx], oob_X_sets[idx], oob_y_sets[idx]) for idx in range(number_of_sets) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3 (2 балла)\n",
    "Оптимизируйте по `AUC` на кроссвалидации (размер валидационной выборки - 20%) параметры своей реализации `Random Forest`: \n",
    "\n",
    "максимальную глубину деревьев из [2, 3, 5, 7, 10], количество деревьев из [5, 10, 20, 30, 50, 100]. \n",
    "\n",
    "Постройте `ROC` кривую (и выведите `AUC` и `accuracy`) для лучшего варианта.\n",
    "\n",
    "Подсказка: можно построить сразу 100 деревьев глубины 10, а потом убирать деревья и\n",
    "глубину."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X: np.array, y: np.array, ratio: float = 0.7, cv_ratio: float = 0):\n",
    "    # Возвращает X_train, y_train, X_test, y_test\n",
    "    # X_train и X_test - массив векторов - две части массива X, разделенного в состветсви с коэффициентом ratio\n",
    "    # y_train и y_test - соответствующие X_train и X_test метки классов\n",
    "    if cv_ratio == 0:\n",
    "        treshold: int = int(X.shape[0] * ratio)\n",
    "        return X[:treshold], y[:treshold], X[treshold:], y[treshold:]\n",
    "    else:\n",
    "        train_treshold = int(X.shape[0] * ratio)\n",
    "        cv_treshold = int(X.shape[0] * (cv_ratio + ratio))\n",
    "        return X[:train_treshold], y[:train_treshold], \\\n",
    "            X[train_treshold : cv_treshold], y[train_treshold : cv_treshold], \\\n",
    "            X[cv_treshold:], y[cv_treshold:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = pd.read_csv('hw_trees_data/x_spam_train.csv').to_numpy(), pd.read_csv('hw_trees_data/y_spam_train.csv').to_numpy()\n",
    "X_train, y_train, X_cv, y_cv, X_test, y_test = train_test_split(X, y, 0.5, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(max_depth=10, n_estimators=100)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve(y_test, p_pred, plot=True):\n",
    "    positive_samples = sum(1 for y in y_test if y == 0)\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    for w in np.arange(-0.01, 1.02, 0.01):\n",
    "        y_pred = [(0 if p[0] > w else 1) for p in p_pred]\n",
    "        tpr.append(sum(1 for yp, yt in zip(y_pred, y_test) if yp == 0 and yt == 0) / positive_samples)\n",
    "        fpr.append(sum(1 for yp, yt in zip(y_pred, y_test) if yp == 0 and yt != 0) / (len(y_test) - positive_samples))\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize = (7, 7))\n",
    "        plt.plot(fpr, tpr)\n",
    "        plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "        plt.xlabel(\"False positive rate\")\n",
    "        plt.ylabel(\"True positive rate\")\n",
    "        plt.xlim(-0.01, 1.01)\n",
    "        plt.ylim(-0.01, 1.01)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return fpr, tpr\n",
    "\n",
    "def compute_auc(x, y):\n",
    "    return -np.trapz(y, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc = None\n",
    "best_tree_number = 100\n",
    "best_max_depth = 10\n",
    "y_true = [ y[1] for y in y_cv ]\n",
    "\n",
    "for max_depth in [ 2, 3, 5, 7, 10 ][::-1]:\n",
    "    for n_trees in [ 5, 10, 20, 30, 50, 100 ][::-1]:\n",
    "        y_pred = forest.predict_proba(X_cv, n_trees, max_depth)\n",
    "\n",
    "        fpr, tpr = roc_curve(y_true, y_pred, plot=False)\n",
    "        auc = compute_auc(fpr, tpr)\n",
    "\n",
    "        if best_auc is None or best_auc < auc:\n",
    "            best_auc = auc\n",
    "            best_tree_number = n_trees\n",
    "            best_max_depth = max_depth\n",
    "\n",
    "print(best_tree_number, best_max_depth, best_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_true = [ y[1] for y in y_test ] # Отрезаем колонку ID\n",
    "y_pred = forest.predict(X_test, best_tree_number, best_max_depth)\n",
    "fpr, tpr = roc_curve([ y[1] for y in y_test ], forest.predict_proba(X_test, best_tree_number, best_max_depth))\n",
    "print(accuracy_score(y_true, y_pred), compute_auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто хочется понимать, насколько большую роль играет тот или иной признак для предсказания класса объекта. Есть различные способы посчитать его важность. Один из простых способов сделать это для Random Forest выглядит так:\n",
    "1. Посчитать out-of-bag ошибку предсказания `err_oob` (https://en.wikipedia.org/wiki/Out-of-bag_error)\n",
    "2. Перемешать значения признака `j` у объектов выборки (у каждого из объектов изменится значение признака `j` на какой-то другой)\n",
    "3. Посчитать out-of-bag ошибку (`err_oob_j`) еще раз.\n",
    "4. Оценкой важности признака `j` для одного дерева будет разность `err_oob_j - err_oob`, важность для всего леса считается как среднее значение важности по деревьям.\n",
    "\n",
    "Реализуйте функцию `feature_importance`, которая принимает на вход Random Forest и возвращает массив, в котором содержится важность для каждого признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(rfc):\n",
    "    importance = np.empty((len(X_train[0])))\n",
    "    oob_error = rfc.compute_oob_error()\n",
    "    \n",
    "    for feature_idx in range(1, len(X_train[0])): # Skip ID\n",
    "        forest = rfc.clone()\n",
    "        X = np.transpose(X_train)\n",
    "        X[feature_idx] = np.random.shuffle(X[feature_idx])\n",
    "        forest.fit(np.transpose(X), y_train)\n",
    "        importance[feature_idx] = forest.compute_oob_error() - oob_error\n",
    "\n",
    "\n",
    "def most_important_features(importance, names, k=20):\n",
    "    # Выводит названия k самых важных признаков\n",
    "    idicies = np.argsort(importance)[::-1][:k] + 1 # +1 потому что отрезали первую фичу: ID\n",
    "    return np.array(names)[idicies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте решение на простом синтетическом наборе данных. В результате должна получиться точность `1.0`, наибольшее значение важности должно быть у признака с индексом `4`, признаки с индексами `2` и `3`  должны быть одинаково важны, а остальные признаки - не важны совсем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_dataset(size):\n",
    "    X = [(np.random.randint(0, 2), np.random.randint(0, 2), i % 6 == 3, \n",
    "          i % 6 == 0, i % 3 == 2, np.random.randint(0, 2)) for i in range(size)]\n",
    "    y = [i % 3 for i in range(size)]\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = synthetic_dataset(1000)\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(np.array([ np.array([ idx ] + x) for idx, x in enumerate(X) ]), np.array([ np.array([ idx, i ]) for idx, i in enumerate(y) ]))\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(np.array([ np.array([ idx ] + x) for idx, x in enumerate(X) ])) == y))\n",
    "# print(\"Importance:\", feature_importance(rfc)) # Очень долго считается, комп греется, но должно работать :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте, какие признаки важны для датасета spam? (Используйте файлы x_spam_train и y_spam_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = pd.read_csv('hw_trees_data/x_spam_train.csv').to_numpy(), pd.read_csv('hw_trees_data/y_spam_train.csv').to_numpy()\n",
    "forest = RandomForestClassifier(max_depth=best_max_depth, n_estimators=best_tree_number)\n",
    "forest.fit(X, y)\n",
    "# print(\"Importance:\", feature_importance(forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ваш ответ_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. Обучите модель на всех данных из x_spam_train и y_spam_train.\n",
    "2. Сделайте submit своего решения и получите значение f1_score не менее 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('hw_trees_data/x_spam_test.csv')\n",
    "\n",
    "submission = pd.DataFrame(columns = [\"Id\", \"Expected\"])\n",
    "submission[\"Id\"] = test[\"Id\"]\n",
    "submission[\"Expected\"] = forest.predict(test.to_numpy(), best_tree_number, best_max_depth)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве альтернативы попробуем библиотечные реализации ансамблей моделей. \n",
    "\n",
    "1. [CatBoost](https://catboost.ai/docs/)\n",
    "2. [XGBoost](https://xgboost.readthedocs.io/en/latest/)\n",
    "3. [LightGBM](https://lightgbm.readthedocs.io/en/latest/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установите необходимые библиотеки. \n",
    "Возможно, потребуется установка дополнительных пакетов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightgbm\n",
    "# !pip install catboost\n",
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Примените модели для нашего датасета.\n",
    "\n",
    "2. Для стандартного набора параметров у каждой модели нарисуйте `ROC` кривую и выведите `AUC` и `accuracy`.\n",
    "\n",
    "3. Посчитайте время обучения каждой модели (можно использовать [timeit magic](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit)).\n",
    "\n",
    "4. Сравните метрики качества и скорость обучения моделей. Какие выводы можно сделать?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import auc\n",
    "import time\n",
    "\n",
    "y_true = [ y[1] for y in y_test ]\n",
    "\n",
    "\n",
    "cat = CatBoostClassifier()\n",
    "start_time = time.time_ns()\n",
    "cat.fit(X_train, [ i[1] for i in y_train ], logging_level='Silent')\n",
    "end_time = time.time_ns()\n",
    "fpr, tpr = roc_curve(y_true, cat.predict_proba(X_test))\n",
    "print(accuracy_score(y_true, cat.predict(X_test)), (end_time - start_time) / 1_000_000, auc(fpr, tpr))\n",
    "\n",
    "light = LGBMClassifier()\n",
    "start_time = time.time_ns()\n",
    "light.fit(X_train, [ i[1] for i in y_train ])\n",
    "end_time = time.time_ns()\n",
    "fpr, tpr = roc_curve(y_true, light.predict_proba(X_test))\n",
    "print(accuracy_score(y_true, light.predict(X_test)), (end_time - start_time) / 1_000_000, auc(fpr, tpr))\n",
    "\n",
    "xgbc = XGBClassifier()\n",
    "start_time = time.time_ns()\n",
    "xgbc.fit(X_train, [ i[1] for i in y_train ])\n",
    "end_time = time.time_ns()\n",
    "fpr, tpr = roc_curve(y_true, xgbc.predict_proba(X_test))\n",
    "print(accuracy_score(y_true, xgbc.predict(X_test)), (end_time - start_time) / 1_000_000, auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ваш ответ_: Модели имеют примерно одинаковое время обучения и результирующие метрики. Кажется, что наилучшим результатом обладает cat_boost, который обучается чуть быстрее остальных (100ms) и при этом имеет accuracy и AUC на 0.01 выше"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d22865cb200c2dfdd3f5854500180a39270a2dc4977b5698ff256ed7c9469f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
